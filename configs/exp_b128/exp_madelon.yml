optimizers:
  scinol:
  - {}
  - {beta: 1}
  - {epsilon_scaled: True}
  scinol2:
  - {epsilon_scaled: True}
  - {}
  scinola:
  scinol2a:
  scinolb:
  scinol2b:
  prescinol:
  - {}
  - {epsilon_scaled: 'd'}
  - {epsilon_scaled: 'dt'}
  prescinol2:
  adam:
    - {learning_rate: 1.0 }
    - {learning_rate: 0.1 }
    - {learning_rate: 0.5 }
    - {learning_rate: 0.01 }
    - {learning_rate: 0.05 }
    - {learning_rate: 0.001 }
    - {learning_rate: 0.005 }
    - {learning_rate: 0.0001 }
    - {learning_rate: 0.0005 }
    - {learning_rate: 0.00001 }
  adagrad:
    - {learning_rate: 1.0 }
    - {learning_rate: 0.1 }
    - {learning_rate: 0.5 }
    - {learning_rate: 0.01 }
    - {learning_rate: 0.05 }
    - {learning_rate: 0.001 }
    - {learning_rate: 0.005 }
    - {learning_rate: 0.0001 }
    - {learning_rate: 0.0005 }
    - {learning_rate: 0.00001 }
  adadelta:
    - {learning_rate: 1.0 }
    - {learning_rate: 0.1 }
    - {learning_rate: 0.5 }
    - {learning_rate: 0.01 }
    - {learning_rate: 0.05 }
    - {learning_rate: 0.001 }
    - {learning_rate: 0.005 }
    - {learning_rate: 0.0001 }
    - {learning_rate: 0.0005 }
    - {learning_rate: 0.00001 }
  rmsprop:
    - {learning_rate: 1.0 }
    - {learning_rate: 0.1 }
    - {learning_rate: 0.5 }
    - {learning_rate: 0.01 }
    - {learning_rate: 0.05 }
    - {learning_rate: 0.001 }
    - {learning_rate: 0.005 }
    - {learning_rate: 0.0001 }
    - {learning_rate: 0.0005 }
    - {learning_rate: 0.00001 }
  sgd:
    - {learning_rate: 1.0 }
    - {learning_rate: 0.1 }
    - {learning_rate: 0.5 }
    - {learning_rate: 0.01 }
    - {learning_rate: 0.05 }
    - {learning_rate: 0.001 }
    - {learning_rate: 0.005 }
    - {learning_rate: 0.0001 }
    - {learning_rate: 0.0005 }
    - {learning_rate: 0.00001 }
  nag:
    - {learning_rate: 1.0 }
    - {learning_rate: 0.1 }
    - {learning_rate: 0.5 }
    - {learning_rate: 0.01 }
    - {learning_rate: 0.05 }
    - {learning_rate: 0.001 }
    - {learning_rate: 0.005 }
    - {learning_rate: 0.0001 }
    - {learning_rate: 0.0005 }
    - {learning_rate: 0.00001 }

models:
  LR:
    - {init0: True}

datasets:
  - UCI_Madelon



train_batchsize: 128
epochs: 60
times: 1
train_histograms: False
train_logs: False
no_tqdm: True