optimizers:
  scinol:
  scinol2:
#  prescinol:
#  - {epsilon_scaled: 'dt'}
#  cocob:
#  adam:
#    - {learning_rate: 1.0 }
#    - {learning_rate: 0.1 }
#    - {learning_rate: 0.01 }
#    - {learning_rate: 0.001 }
#    - {learning_rate: 0.0001 }
#    - {learning_rate: 0.00001 }
#  adagrad:
#    - {learning_rate: 1.0 }
#    - {learning_rate: 0.1 }
#    - {learning_rate: 0.01 }
#    - {learning_rate: 0.001 }
#    - {learning_rate: 0.0001 }
#    - {learning_rate: 0.00001 }
#  rmsprop:
#    - {learning_rate: 1.0 }
#    - {learning_rate: 0.1 }
#    - {learning_rate: 0.01 }
#    - {learning_rate: 0.001 }
#    - {learning_rate: 0.0001 }
#    - {learning_rate: 0.00001 }
#  adadelta:
#    - {learning_rate: 1.0 }
#    - {learning_rate: 0.1 }
#    - {learning_rate: 0.01 }
#    - {learning_rate: 0.001 }
#    - {learning_rate: 0.0001 }
#    - {learning_rate: 0.00001 }
#  sgd:
#    - {learning_rate: 1.0 }
#    - {learning_rate: 0.1 }
#    - {learning_rate: 0.01 }
#    - {learning_rate: 0.001 }
#    - {learning_rate: 0.0001 }
#    - {learning_rate: 0.00001 }
#  nag:
#    - {learning_rate: 1.0 }
#    - {learning_rate: 0.1 }
#    - {learning_rate: 0.01 }
#    - {learning_rate: 0.001 }
#    - {learning_rate: 0.0001 }
#    - {learning_rate: 0.00001 }

models:
  LR:
    - {init0: True}


datasets:
  - Artificial


train_batchsize: 1
epochs: 1
test_every: 50
times: 1
train_histograms: False
train_logs: False
no_tqdm: False
tblogdir: tb_logs_art