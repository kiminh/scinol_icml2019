optimizers:
  scinol2:
  cocob:
  adam:
    - {learning_rate: 1.0 }
    - {learning_rate: 0.1 }
    - {learning_rate: 0.5 }
    - {learning_rate: 0.01 }
    - {learning_rate: 0.05 }
    - {learning_rate: 0.001 }
    - {learning_rate: 0.005 }
    - {learning_rate: 0.0001 }
    - {learning_rate: 0.0005 }
    - {learning_rate: 0.00001 }
  adagrad:
    - {learning_rate: 1.0 }
    - {learning_rate: 0.1 }
    - {learning_rate: 0.5 }
    - {learning_rate: 0.01 }
    - {learning_rate: 0.05 }
    - {learning_rate: 0.001 }
    - {learning_rate: 0.005 }
    - {learning_rate: 0.0001 }
    - {learning_rate: 0.0005 }
    - {learning_rate: 0.00001 }
  adadelta:
    - {learning_rate: 1.0 }
    - {learning_rate: 0.1 }
    - {learning_rate: 0.5 }
    - {learning_rate: 0.01 }
    - {learning_rate: 0.05 }
    - {learning_rate: 0.001 }
    - {learning_rate: 0.005 }
    - {learning_rate: 0.0001 }
    - {learning_rate: 0.0005 }
    - {learning_rate: 0.00001 }
  rmsprop:
    - {learning_rate: 1.0 }
    - {learning_rate: 0.1 }
    - {learning_rate: 0.5 }
    - {learning_rate: 0.01 }
    - {learning_rate: 0.05 }
    - {learning_rate: 0.001 }
    - {learning_rate: 0.005 }
    - {learning_rate: 0.0001 }
    - {learning_rate: 0.0005 }
    - {learning_rate: 0.00001 }
  sgd:
    - {learning_rate: 1.0 }
    - {learning_rate: 0.1 }
    - {learning_rate: 0.5 }
    - {learning_rate: 0.01 }
    - {learning_rate: 0.05 }
    - {learning_rate: 0.001 }
    - {learning_rate: 0.005 }
    - {learning_rate: 0.0001 }
    - {learning_rate: 0.0005 }
    - {learning_rate: 0.00001 }

models:
  NN:
    - { layers: [1000,1000], dropout: 0.9 }




datasets:
  - UCI_Madelon



train_batchsize: 128
epochs: 60
times: 1
train_histograms: False
train_logs: False
no_tqdm: True
tblogdir: tb_logs_nn_b128