optimizers:
#  scinol:
#    - {}
#    - {beta: 1}
#  scinol2:
#  cocob:
#  adam:
  rmsprop:
    - { learning_rate: 2e-3 }
models:
  CharLSTM:
    - {layers: [128,128]}
datasets:
  - WarAndPeace

train_batchsize: 256
epochs: 200
times: 1
train_histograms: False
train_logs: True
embedding_size: 64
