optimizers:
#  scinol:
#    - {}
#    - {beta: 1}
#  scinol2:
#  cocob:
  adam:
#  rmsprop:
#    - { learning_rate: 2e-3 }
models:
  CharLSTM:
    - {layers: [128]}
datasets:
  - WarAndPeace

train_batchsize: 256
epochs: 50
times: 1
train_histograms: False
train_logs: False
embedding_size: 64
